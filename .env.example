# Server Configuration
GRPC_SERVER_HOST=0.0.0.0
GRPC_SERVER_PORT=50051
REST_SERVER_HOST=0.0.0.0
REST_SERVER_PORT=8000

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_STREAM_KEY=stream_process_queue
REDIS_CONSUMER_GROUP=workers

# AWS/SQS Configuration (Optional)
USE_SQS=false
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
SQS_QUEUE_URL=

# Storage Configuration
STORAGE_TYPE=minio  # Options: minio, s3, local
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=stream-process
USE_SSL=false

# Model Configuration
WHISPER_MODEL_SIZE=base  # Options: tiny, base, small, medium, large
WHISPER_DEVICE=cpu  # Options: cpu, cuda
WHISPER_COMPUTE_TYPE=int8  # Options: int8, float16, float32
OCR_LANG=ch_sim,en  # Supported languages for OCR
OCR_USE_GPU=false

# Worker Configuration
WORKER_CONCURRENCY=4
WORKER_MAX_RETRIES=3
WORKER_RETRY_DELAY=5
BATCH_SIZE=8
BATCH_TIMEOUT=1.0

# Performance Settings
MAX_CONCURRENT_STREAMS=100
STREAM_BUFFER_SIZE=8192
CHUNK_DURATION_MS=100
GPU_MEMORY_FRACTION=0.8

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090
LOG_LEVEL=INFO
LOG_FORMAT=json

# Triton Inference Server (Optional)
USE_TRITON=false
TRITON_SERVER_URL=localhost:8001
TRITON_MODEL_NAME=whisper_onnx
TRITON_MODEL_VERSION=1